# -*- coding: utf-8 -*-
#__author__='jiahuiyu'
import numpy as np

def softmax_loss_naive(w,x,y,reg):
    # initialize the loss and gradient
    loss = 0
    dw = np.zeros_like(w)
    dw_each = np.zeros_like(w)
    num_train, dim =x.shape()
    num_class = w.shape[1]
    f = x.dot(w)
    # 找到每行最大值，并减去最大值
    f_max = np.reshape(np.max(f, axis=1), (num_train, 1))
    prob = np.exp(f - f_max) / np.sum(np.exp(f - f_max),axis =1 , keepdims =True)
    # 初始化
    y_trueclass = np.zeros_like(prob)
    y_trueclass[np.arange(num_train),y] = 1.0
    for i in xrange(num_train):
        for j in xrange(num_class):
            loss += -(y_trueclass[i,j] * np.log(prob[i,j])) #损失函数
            dw_each[:,j] = -(y_trueclass[i,j] - prob[i,j]) * x[i,:] # 梯度计算
        dw += dw_each  # 把每个类的loss放在一起
    loss /=num_train
    loss += 0.5 * reg * np.sum(w*w) # 加上正则表达式
    dw /= num_train
    dw += reg *w

    return loss ,dw



